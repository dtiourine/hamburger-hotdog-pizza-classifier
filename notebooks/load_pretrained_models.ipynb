{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Loading Pretrained Models\n",
    "\n",
    "In this notebook, we experiment with different pretrained models like `VGG16` and `AlexNet`. For each model we will train it on a subset of the data and observe its performance on the corresponding validation set. From there, we will take the best performing model and finetune it further by training it on the entire dataset in `fine_tuning.ipynb`."
   ],
   "id": "d49871b0903be241"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:21.959619Z",
     "start_time": "2024-07-26T16:51:19.931867Z"
    }
   },
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import time\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# torchvision imports\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import vgg16, VGG16_Weights, resnet50, ResNet50_Weights, alexnet, AlexNet_Weights\n",
    "\n",
    "# Project-specific imports\n",
    "from hamburger_hotdog_pizza_classifier.config import PROCESSED_DATA_DIR, MODELS_DIR, BATCH_SIZE, MOMENTUM, NUM_CLASSES, LEARNING_RATE\n",
    "from utils import train_validate_model#, modify_model_output\n",
    "\n",
    "# Logging and experiment tracking\n",
    "from loguru import logger\n",
    "\n",
    "# Load hyperparameters\n",
    "batch_size = BATCH_SIZE\n",
    "lr = LEARNING_RATE\n",
    "momentum = MOMENTUM\n",
    "num_classes = NUM_CLASSES\n",
    "\n",
    "print('Imported necessary libraries/variables')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2024-07-26 12:51:21.954\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mhamburger_hotdog_pizza_classifier.config\u001B[0m:\u001B[36m<module>\u001B[0m:\u001B[36m10\u001B[0m - \u001B[1mPROJ_ROOT path is: C:\\Git\\hamburger-hotdog-pizza-classifier\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported necessary libraries/variables\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:21.963231Z",
     "start_time": "2024-07-26T16:51:21.960625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Directory paths\n",
    "subset_image_path = PROCESSED_DATA_DIR / \"pizza_hamburger_hotdog_20_percent\"\n",
    "train_dir = subset_image_path / 'train'\n",
    "test_dir = subset_image_path / 'test'\n",
    "valid_dir = subset_image_path / 'valid'\n",
    "\n",
    "# Data loading parameters\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "transform = weights.transforms()\n",
    "\n",
    "# Loss function setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "id": "40cbf3f3d5ba9ef3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Start with Pre-Trained Models\n",
    "\n",
    "For this dataset, we will experiment with `Resnet50`, `VGG16`, and `AlexNet` \n",
    "\n",
    "Let's choose some popular CNN architectures as a starting point\n",
    "\n",
    "First, we need to load these pretrained models"
   ],
   "id": "bba17d20cc5aec51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:51:21.968111Z",
     "start_time": "2024-07-26T16:51:21.963231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# alex_net = modify_model_output('alexnet', num_classes, device)\n",
    "# vgg16 = modify_model_output('vgg16', num_classes, device)\n",
    "# resnet50 = modify_model_output('resnet50', num_classes, device)"
   ],
   "id": "912760e86fdc75de",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Trying ResNet50",
   "id": "1640c201714da7c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:25.699644Z",
     "start_time": "2024-07-26T18:28:25.062727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up weights and modify ResNet50 model\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "resnet50_model = resnet50(weights=weights)\n",
    "resnet50_model = resnet50_model.to(device)\n",
    "\n",
    "# Adjust the final fully connected layer to match the number of classes\n",
    "resnet50_model.fc = nn.Linear(resnet50_model.fc.in_features, num_classes)"
   ],
   "id": "a98b6ee54d7cd9e7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:26.147575Z",
     "start_time": "2024-07-26T18:28:26.142065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transform setup for ResNet50\n",
    "resnet50_transform = weights.transforms()\n",
    "\n",
    "# Data preparation\n",
    "train_data_resnet = ImageFolder(train_dir, transform=resnet50_transform)\n",
    "valid_data_resnet = ImageFolder(valid_dir, transform=resnet50_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader_resnet = DataLoader(train_data_resnet, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_resnet = DataLoader(valid_data_resnet, batch_size=batch_size, shuffle=True)"
   ],
   "id": "a4d963ad6ce48137",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:26.704840Z",
     "start_time": "2024-07-26T18:28:26.701830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "device = next(resnet50_model.parameters()).device\n",
    "print(f\"The model is on device: {device}\")"
   ],
   "id": "a1b4f16f348d1818",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on device: cuda:0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T18:28:27.939646Z",
     "start_time": "2024-07-26T18:28:27.518817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimizer and save path setup\n",
    "optimizer = optim.SGD(resnet50_model.parameters(), lr=lr, momentum=momentum)\n",
    "model_save_path = MODELS_DIR / \"slightly_fine_tuned\" / \"resnet50.pth\"\n",
    "\n",
    "# Logging setup\n",
    "model_name = \"ResNet50\"\n",
    "date_time = time.time()\n",
    "logger.add(f\"logs/{model_name}/training_log-{date_time}.log\", format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# Model training and validation\n",
    "train_validate_model(\n",
    "    num_epochs=10, \n",
    "    model=resnet50_model, \n",
    "    train_loader=train_loader_resnet, \n",
    "    valid_loader=valid_loader_resnet, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    model_save_path=model_save_path\n",
    ")"
   ],
   "id": "1ca4305797e855c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images are on device: cuda:0\n",
      "Labels are on device: cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m logger\u001B[38;5;241m.\u001B[39madd(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogs/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/training_log-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.log\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{time}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{level}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{message}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINFO\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Model training and validation\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m train_validate_model(\n\u001B[0;32m     12\u001B[0m     num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, \n\u001B[0;32m     13\u001B[0m     model\u001B[38;5;241m=\u001B[39mresnet50_model, \n\u001B[0;32m     14\u001B[0m     train_loader\u001B[38;5;241m=\u001B[39mtrain_loader_resnet, \n\u001B[0;32m     15\u001B[0m     valid_loader\u001B[38;5;241m=\u001B[39mvalid_loader_resnet, \n\u001B[0;32m     16\u001B[0m     criterion\u001B[38;5;241m=\u001B[39mcriterion, \n\u001B[0;32m     17\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39moptimizer, \n\u001B[0;32m     18\u001B[0m     device\u001B[38;5;241m=\u001B[39mdevice, \n\u001B[0;32m     19\u001B[0m     model_save_path\u001B[38;5;241m=\u001B[39mmodel_save_path\n\u001B[0;32m     20\u001B[0m )\n",
      "File \u001B[1;32mC:\\Git\\hamburger-hotdog-pizza-classifier\\notebooks\\utils.py:27\u001B[0m, in \u001B[0;36mtrain_validate_model\u001B[1;34m(num_epochs, train_loader, valid_loader, model, criterion, optimizer, device, model_save_path)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLabels are on device: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 27\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(images)\n\u001B[0;32m     28\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m     29\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_impl(x)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\models\\resnet.py:280\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    278\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n\u001B[0;32m    279\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mflatten(x, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 280\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc(x)\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Trying VGG16",
   "id": "996b02b0bba27a80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up weights and modify VGG16 model\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg16_model = vgg16(weights=weights)\n",
    "\n",
    "# Adjust the final fully connected layer to match the number of classes\n",
    "vgg16_model.classifier[6] = nn.Linear(vgg16_model.classifier[6].in_features, num_classes)"
   ],
   "id": "f80b6d04ab1b29a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform setup for VGG16\n",
    "vgg16_transform = weights.transforms()\n",
    "\n",
    "# Data preparation\n",
    "train_data_vgg16 = ImageFolder(train_dir, transform=vgg16_transform)\n",
    "valid_data_vgg16 = ImageFolder(valid_dir, transform=vgg16_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader_vgg16 = DataLoader(train_data_vgg16, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_vgg16 = DataLoader(valid_data_vgg16, batch_size=batch_size, shuffle=True)"
   ],
   "id": "e041aadc8411fd9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer and save path setup\n",
    "optimizer = optim.SGD(vgg16_model.parameters(), lr=lr, momentum=momentum)\n",
    "model_save_path = MODELS_DIR / \"slightly_fine_tuned\" / \"vgg16.pth\"\n",
    "\n",
    "# Logging setup\n",
    "model_name = \"VGG16\"\n",
    "date_time = time.time()\n",
    "logger.add(f\"logs/{model_name}/training_log-{date_time}.log\", format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# Model training and validation\n",
    "train_validate_model(\n",
    "    num_epochs=10, \n",
    "    model=vgg16_model, \n",
    "    train_loader=train_loader_vgg16, \n",
    "    valid_loader=valid_loader_vgg16, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    model_save_path=model_save_path\n",
    ") "
   ],
   "id": "feae4c3ca3ae5395",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Trying AlexNet",
   "id": "8acf49b16ceb74e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up weights and modify VGG16 model\n",
    "weights = AlexNet_Weights.DEFAULT\n",
    "alex_net_model = alexnet(weights=weights)\n",
    "\n",
    "# Adjust the final fully connected layer to match the number of classes\n",
    "alex_net_model.classifier[6] = nn.Linear(alex_net_model.classifier[6].in_features, num_classes)"
   ],
   "id": "e8c354cbfcc89fce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Transform setup for AlexNet\n",
    "alex_net_transform = weights.transforms()\n",
    "\n",
    "# Data preparation\n",
    "train_data_alex_net = ImageFolder(train_dir, transform=alex_net_transform)\n",
    "valid_data_alex_net = ImageFolder(valid_dir, transform=alex_net_transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader_alex_net = DataLoader(train_data_alex_net, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_alex_net = DataLoader(valid_data_alex_net, batch_size=batch_size, shuffle=True)"
   ],
   "id": "5e93da92a16aa6ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optimizer and save path setup\n",
    "optimizer = optim.SGD(alex_net_model.parameters(), lr=lr, momentum=momentum)\n",
    "model_save_path = MODELS_DIR / \"slightly_fine_tuned\" / \"alex_net.pth\"\n",
    "\n",
    "# Logging setup\n",
    "model_name = \"AlexNet\"\n",
    "date_time = time.time()\n",
    "logger.add(f\"logs/{model_name}/training_log-{date_time}.log\", format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "# Model training and validation\n",
    "train_validate_model(\n",
    "    num_epochs=10, \n",
    "    model=alex_net_model, \n",
    "    train_loader=train_loader_alex_net, \n",
    "    valid_loader=valid_loader_alex_net, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    device=device, \n",
    "    model_save_path=model_save_path\n",
    ") "
   ],
   "id": "e89b697a03c5228c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since VGG16 was able to achieve nearly 88% accuracy on the validation set, let's start off with VGG16 as our base model to improve upon.",
   "id": "971b10be6cb0ac70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
